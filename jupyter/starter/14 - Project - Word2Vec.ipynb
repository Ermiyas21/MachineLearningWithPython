{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b13b1e",
   "metadata": {},
   "source": [
    "# Project: Create a Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b981a3",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27464c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a98c3250",
   "metadata": {},
   "source": [
    "### Step 2: Download stopwords\n",
    "- Execute the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b371d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9012c520",
   "metadata": {},
   "source": [
    "### Step 3: Read content and sentinize\n",
    "- Initialize an empty list called **all_sentences**\n",
    "- For each filename in **'files/holmes'**:\n",
    "    - HINT: Use **os.listdir(...)** ([docs](https://docs.python.org/3/library/os.html#os.listdir))\n",
    "- Open the file and read the content and convert to lowercase and apply **nltk.sent_tokenize** on content.\n",
    "    - Use **lower()** on content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdbcb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb9808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64e4362c",
   "metadata": {},
   "source": [
    "### Step 4: Tokenize each sentence\n",
    "- Get all words by applying **nltk.word_tokenize** on them and assign the result to **all_words**\n",
    "    - HINT: Use list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5694453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb431b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "959014e8",
   "metadata": {},
   "source": [
    "### Step 5: Remove all stop words\n",
    "- Use **stopwords.words('english')** to filter all the words in **all_words**\n",
    "    - HINT: iterate over the length of **all_words**, for each index use list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18213810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abbfb77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40656352",
   "metadata": {},
   "source": [
    "### Step 6: Remove special characters\n",
    "- Iterate over items in **all_words** to remove words with special characters\n",
    "    - HINT: Use **isalpha()** ([doc](https://docs.python.org/3/library/stdtypes.html#str.isalpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c2c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7126656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8bca8cf",
   "metadata": {},
   "source": [
    "### Step 7: Install gensim and python-Levenshtein\n",
    "- Run the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4acb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a17d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e16a94c5",
   "metadata": {},
   "source": [
    "### Step 8: Import another library\n",
    "- Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62251bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6dad3d2",
   "metadata": {},
   "source": [
    "### Step 9: Create a model\n",
    "- Use **Word2Vec** on **all_words**\n",
    "    - Use **min_count=2** : Ignores all words with total frequency lower than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18b227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97a79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1da22183",
   "metadata": {},
   "source": [
    "### Step 10: Find distances\n",
    "- Try to run **model.wv.distance('holmes', 'watson')**\n",
    "- Try to run **model.wv.distance('holmes', 'water')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9175dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96ec48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b26dd8c",
   "metadata": {},
   "source": [
    "### Step 11: Find closests words\n",
    "- Get all the words\n",
    "    - HINT: **words = model.wv.index_to_key**\n",
    "- Implement a function **closets_words(word)**\n",
    "    - HINT: **distances = {w: model.wv.distance(word, w) for w in words}**\n",
    "    - HINT: **sorted(distances, key=lambda w: distances[w])[:15]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff11524d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e29f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f2f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
